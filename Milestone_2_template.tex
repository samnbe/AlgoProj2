\documentclass{article}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{float}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{hyperref}

\pgfplotsset{compat=1.14}
\begin{document}

\title{COP4533 - Programming Assignment Milestone-2 Report}
\date{}
\maketitle

\section{Team Members}
Samantha Bennett worked on creating and implementing each algorithm and their proofs. Sophia Shah worked on creating each algorithm, the comparative study, and the algorithm analysis.

\section{Algorithm Design and Analysis}

\subsection{Algorithm 3}
\subsubsection{Description}

We can solve the collecting treasure problem using a recursive approach \textbf{without} memoization.

\textbf{Recursive strategy:} \\
At every vault index $i$, the algorithm computes the maximum value that can be collected by considering two options:
\begin{enumerate}
    \item \textbf{Include} the current vault $i$, and skip the next $k$ vaults by calling the recursive function on index $i - k - 1$.
    \item \textbf{Exclude} the current vault $i$, and instead call the recursive function on index $i - 1$.
\end{enumerate}
The recursive function then returns the maximum of these two values.

\textbf{Step-by-step example:} \\
Suppose we have the list of treasure vault values and $k = 2$:
\[
[1, 2, 3, 4, 5, 6, 7].
\]
\begin{enumerate}
    \item \textbf{Base case:}
    \[
    OPT[0] = 0.
    \]
    When there are no vaults, the maximum value is $0$.

    \item \textbf{For $i = 1$:}
    \[
    OPT[1] = \max(values[1] + OPT[-2],\; OPT[0]) = \max(1 + 0,\; 0) = 1.
    \]
    So we take vault $1$.

    \item \textbf{For $i = 2$:}
    \[
    OPT[2] = \max(values[2] + OPT[-1],\; OPT[1]) = \max(2 + 0,\; 1) = 2.
    \]
    Vault $2$ is better than vault $1$ alone.

    \item \textbf{For $i = 3$:}
    \[
    OPT[3] = \max(values[3] + OPT[0],\; OPT[2]) = \max(3 + 0,\; 2) = 3.
    \]
    We take vault $3$.

    \item \textbf{For $i = 4$:}
    \[
    OPT[4] = \max(values[4] + OPT[1],\; OPT[3]) = \max(4 + 1,\; 3) = 5.
    \]
    The best combination up to vault $4$ is vaults $1$ and $4$.

    \item \textbf{For $i = 5$:}
    \[
    OPT[5] = \max(values[5] + OPT[2],\; OPT[4]) = \max(5 + 2,\; 5) = 7.
    \]
    The optimal set up to vault $5$ is vaults $2$ and $5$.

    \item \textbf{For $i = 6$:}
    \[
    OPT[6] = \max(values[6] + OPT[3],\; OPT[5]) = \max(6 + 3,\; 7) = 9.
    \]
    The optimal set up to vault $6$ is vaults $3$ and $6$.

    \item \textbf{For $i = 7$:}
    \[
    OPT[7] = \max(values[7] + OPT[4],\; OPT[6]) = \max(7 + 5,\; 9) = 12.
    \]
    The optimal set up to vault $7$ is vaults $1$, $4$, and $7$.

\end{enumerate}

\textbf{Result:} \\
The final maximum total value is:
\[
OPT[7] = 12.
\]
By backtracking through the computed values, the selected vaults are:
\[
[1, 4, 7].
\]
Hence, the total value collected is $1 + 4 + 7 = 12$.

\vspace{1em}

This algorithm does \textbf{not} use memoization. Therefore, the same subproblems are recomputed many times, leading to an exponential running time.

\begin{algorithm}[H]
\renewcommand{\thealgorithm}{3}
\caption{Finding Maximum Total Vault Value (Recursive without Memoization)}\label{alg:recursive}
\begin{algorithmic}[3]
\Procedure{MaxVault}{$n, k, v_1, v_2, \ldots, v_n$}
    \State $OPT \gets [-1]$ repeated $n + 1$ times
    \State $S \gets \emptyset$ \Comment{set of selected vault indices}
    \Function{ComputeOPT}{$i$}
        \If{$i \leq 0$}
            \State \Return 0
        \EndIf
        \State $include \gets v_i +$ \Call{ComputeOPT}{$i - k - 1$}
        \State $exclude \gets$ \Call{ComputeOPT}{$i - 1$}
        \State $OPT[i] \gets \max(include, exclude)$
        \State \Return $OPT[i]$
    \EndFunction
    \State $TotalValue \gets$ \Call{ComputeOPT}{$n$}
    \State $i \gets n$
    \While{$i > 0$}
        \If{$OPT[i] \neq OPT[i - 1]$}
            \State $S \gets S \cup \{i\}$
            \State $i \gets i - k - 1$
        \Else
            \State $i \gets i - 1$
        \EndIf
    \EndWhile
    \State \Return (TotalValue, $S$)
\EndProcedure
\end{algorithmic}
\end{algorithm}


\subsubsection{Correctness Proof}

\noindent\textbf{Theorem.}  

Let $[v_1, \ldots, v_n]$ be nonnegative vault values and $k \ge 0$ the spacing constraint. Define the recurrence  
\[
\text{OPT}[i] =
\begin{cases}
0, & \text{if } i \le 0, \\
\max\{\text{OPT}[i-1], v_i + \text{OPT}[i-k-1]\}, & \text{otherwise.}
\end{cases}
\]
Then, $\text{OPT}[n]$ equals the maximum achievable total vault value over all feasible selections.  

Using the standard backtrack rule:
\[
\text{If } \text{OPT}[i] \neq \text{OPT}[i-1] \Rightarrow \text{include } i \text{ and set } i \leftarrow i - k - 1; \quad
\text{else set } i \leftarrow i - 1,
\]
then the algorithm returns an optimal feasible set of indices of vaults.

\vspace{1em}

\noindent\textbf{Optimality Proof.} [by induction on $i$]  

Let $\text{opt\_ind}(i)$ be the set of feasible selections using only indices in $\{1, \ldots, i\}$, and let $\text{opt\_total}(i)$ denote the true optimal total value achievable using these indices.  
We define $\text{opt\_total}(i) = 0$ for $i \le 0$, ensuring the recurrence is well-defined when $i - k - 1 \le 0$.

\begin{enumerate}
    \item \textbf{Base case:}  
    When there are no vaults available ($i \le 0$), the maximum possible total value is $0$, so $\text{opt\_total}(i) = 0$.  
    The algorithm sets $\text{OPT}[i] = 0$ in this case, hence $\text{OPT}[i] = \text{opt\_total}(i)$ for all $i \le 0$.
    
    \item \textbf{Inductive step:}  
    Assume for some $x \ge 0$ that $\text{OPT}[j] = \text{opt\_total}(j)$ for all $j \le x$.  
    We will show that $\text{OPT}[x+1] = \text{opt\_total}(x+1)$.
    
    \begin{enumerate}
        \item Let $\text{opt\_ind}^*(x+1)$ achieve $\text{opt\_total}(x+1)$.  
        To obtain the optimal value at $x+1$, there are two possible cases:
        
        \item[(a)] \textbf{Case 1:} Vault $x+1$ is \emph{not selected}.  
        Then $\text{opt\_ind}^*(x+1)$ only includes indices within $\{1, \ldots, x\}$, so the total value is $\text{opt\_total}(x)$.  
        Thus, $\text{opt\_total}(x+1) = \text{opt\_total}(x) = \text{OPT}[x]$.
        
        \item[(b)] \textbf{Case 2:} Vault $x+1$ \emph{is selected}.  
        Because of the $k$-spacing constraint, we cannot select any of the previous $k$ vaults, so the feasible set is $\text{opt\_ind}((x+1) - k - 1)$.  
        Hence, $\text{opt\_total}(x+1) = v_{x+1} + \text{opt\_total}((x+1) - k - 1)$.  
        By the induction hypothesis, $\text{opt\_total}((x+1) - k - 1) = \text{OPT}[(x+1) - k - 1]$,  
        so $\text{opt\_total}(x+1) = v_{x+1} + \text{OPT}[(x+1) - k - 1]$.
    \end{enumerate}
    
    Since these are the only two mutually exclusive and exhaustive possibilities for vault $x+1$, the optimal value at index $x+1$ is obtained by taking the maximum of these two cases:
    \[
    \text{opt\_total}(x+1) = \max\{\text{OPT}[x],\, v_{x+1} + \text{OPT}[(x+1) - k - 1]\} = \text{OPT}[x+1].
    \]
    This completes the induction.
\end{enumerate}

By induction, $\text{OPT}[n] = \text{opt\_total}(n)$, so the algorithm correctly computes the maximum total vault value.  
Therefore, the total value given by this algorithm is \textbf{optimal}.

\vspace{1em}

\noindent\textbf{Feasibility Proof.}  

To show that the backtracking reconstruction produces a feasible set (one that satisfies the $k$-spacing constraint), we proceed as follows:

\begin{enumerate}
    \item Initialize $i = n$ and run the backtrack procedure:  

    \begin{enumerate}
        \item If $\text{OPT}[i] = \text{OPT}[i-1]$, then the exclude-branch yields the optimal total at $i$, meaning that $i$ is not included in any optimal solution. We then set $i \leftarrow i - 1$.

        \item If $\text{OPT}[i] \neq \text{OPT}[i-1]$, then by the recurrence relation we have $\text{OPT}[i] = v_i + \text{OPT}[i-k-1]$.  
        Thus, some optimal solution includes vault $i$. We record index $i$ and then set $i \leftarrow i - k - 1$.


        \item Because of this jump by $k + 1$ positions, no index within the interval $[i - k, \ldots, i - 1]$ can be included in the solution.  
        Therefore, all selected indices differ by at least $k + 1$.
    \end{enumerate}
 
\end{enumerate}

Hence, the reconstructed solution satisfies the $k$-spacing constraint and is feasible.  
At each step, the inclusion or exclusion of $i$ aligns with the recurrence defining $\text{OPT}$, ensuring the recorded indices yield exactly $\text{OPT}[n]$.  

\vspace{1em}

\noindent\textbf{Conclusion:}  
Both the feasibility and optimality proofs demonstrate that the dynamic programming algorithm produces a valid (feasible) and optimal set of vault selections under the $k$-spacing constraint.  
Therefore, the algorithm is \textbf{correct}.


\subsubsection{Runtime Analysis}

Let $n$ be the number of vault values in the list.  
The recursive algorithm evaluates every possible combination of vault selections under the $k$-spacing constraint.  
At each index $i$, the algorithm makes two recursive calls:
\begin{enumerate}
    \item One call that \textbf{includes} the current vault $i$ and then proceeds to index $(i - k - 1)$.
    \item One call that \textbf{excludes} the current vault $i$ and proceeds to index $(i - 1)$.
\end{enumerate}

This results in a binary branching structure of recursive calls.  
Let $T(n)$ denote the number of operations required to compute the result for $n$ vaults.  
The recurrence relation can be expressed as:

\[
T(n) = T(n - 1) + T(n - k - 1) + c,
\]
where $c$ is the constant time required for the comparison and addition at each step.

In the worst case, when $k$ is small ($k = 0$ or $k = 1$), the recurrence simplifies to approximately:

\[
T(n) = T(n - 1) + T(n - 2) + c.
\]

Therefore, the closed-form growth of $T(n)$ is exponential:

\[
T(n) = O(2^n).
\]

Even for larger $k$, the recursive tree still exhibits exponential growth because the number of unique subproblems increases exponentially without memoization.  
Each call recomputes results for overlapping subproblems many times.

Thus, the overall running time of the recursive vault-selection algorithm \textbf{without memoization} is:

\[
T(n, k) = O(2^n).
\]

If memoization or dynamic programming were applied, the number of unique subproblems would be reduced to $n$, resulting in a polynomial runtime.  
However, in this unoptimized recursive version, every subproblem is recalculated repeatedly, leading to exponential time complexity.


\subsection{Algorithm 4}
\subsubsection{Description}

We can solve the collecting treasure problem using dynamic programming.  
In this approach, each subproblem corresponds to finding the maximum total vault value obtainable from the first $i$ vaults under the $k$-spacing constraint.  

The recursive relation for the problem is defined as:

\[
OPT[i] =
\begin{cases}
0, & \text{if } i = 0, \\[6pt]
\displaystyle \max_{1 \leq j \leq i} \{ v_j + OPT[j - k - 1] \}, & \text{otherwise.}
\end{cases}
\]

Intuitively, for each position $i$, we examine every vault $j$ up to $i$ and consider including vault $j$ in the optimal subset.  
If vault $j$ is chosen, all vaults from $(j - k)$ through $(j - 1)$ must be excluded, and the best total including $v_j$ becomes $v_j + OPT[j - k - 1]$.  
The maximum of all such choices for vault $j$ yields $OPT[i]$.

\vspace{1em}

\textbf{Algorithm 4A (Top-Down Recursive DP with Memoization):} \\
Algorithm 4A implements this recurrence recursively.  
The function computes $OPT[i]$ using memoization to store previously computed values in the array \texttt{OPT}.  
Whenever a new subproblem is encountered, it explores all $j$ from $1$ to $i$, updating the maximum total value.  
Once computed, $OPT[i]$ is stored to avoid redundant recursive calls.  
After computing $OPT[n]$, the algorithm backtracks to reconstruct the chosen vault indices.

\begin{algorithm}[H]
\renewcommand{\thealgorithm}{4A}
\caption{Finding Maximum Total Vault Value (Top-Down Recursive DP with Memoization)}\label{alg:topdown}
\begin{algorithmic}[1]
\Procedure{MaxVault-TopDown}{$n, k, v_1, v_2, \ldots, v_n$}
    \State $OPT \gets [-1]$ repeated $n + 1$ times
    \State $S \gets \emptyset$ \Comment{set of selected vault indices}
    \Function{ComputeOPT}{$i$}
        \If{$i \leq 0$}
            \State \Return 0
        \EndIf
        \If{$OPT[i] \neq -1$}
            \State \Return $OPT[i]$
        \EndIf
        \State $exclude \gets$ \Call{ComputeOPT}{$i - 1$}
        \State $best \gets exclude$
        \For{$j = 1$ to $i$}
            \State $include \gets v_j$
            \If{$j - k - 1 > 0$}
                \State $include \gets include +$ \Call{ComputeOPT}{$j - k - 1$}
            \EndIf
            \State $best \gets \max(best, include)$
        \EndFor
        \State $OPT[i] \gets best$
        \State \Return $OPT[i]$
    \EndFunction
    \State $TotalValue \gets$ \Call{ComputeOPT}{$n$}
    \State $i \gets n$
    \While{$i > 0$}
        \If{$OPT[i] \neq OPT[i - 1]$}
            \State $S \gets S \cup \{i\}$
            \State $i \gets i - k - 1$
        \Else
            \State $i \gets i - 1$
        \EndIf
    \EndWhile
    \State \Return ($TotalValue$, $S$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\vspace{1em}

\textbf{Step-by-step example:} \\
Suppose we have the list of vault values and $k = 2$:
\[
[1, 2, 3, 4, 5, 6, 7].
\]

We use the recurrence relation:
\[
OPT[i] =
\begin{cases}
0, & \text{if } i = 0, \\[6pt]
\displaystyle \max_{1 \leq j \leq i} \{ v_j + OPT[j - k - 1] \}, & \text{otherwise.}
\end{cases}
\]

\textbf{Step-by-step computation:}
\[
k = 2
\]

\begin{enumerate}
    \item $OPT[0] = 0$
    \item $OPT[1] = \max(v_1) = 1$
    \item $OPT[2] = \max(v_1, v_2) = \max(1,2) = 2$
    \item $OPT[3] = \max(v_1, v_2, v_3) = \max(1, 2, 3) = 3$
    \item $OPT[4] = \max(v_1, v_2, v_3, v_4 + OPT[1]) = \max(1, 2, 3, 4 + 1) = 5$
    \item $OPT[5] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2]) = \max(1, 2, 3, 4+1, 5 + 2) = 7$
    \item $OPT[6] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2], v_6 + OPT[3]) = \max(1, 2, 3, 4+1, 5+2, 6 + 3) = 9$
    \item $OPT[7] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2], v_6 + OPT[3], v_7 + OPT[4]) = \max(1, 2, 3, 4+1, 5+2, 6+3, 7 + 5) = 12$
\end{enumerate}

At the end of the recursive computation:
\[
OPT = [0, 1, 2, 3, 5, 7, 9, 12].
\]

\textbf{Backtracking:}
\begin{itemize}
    \item Start at $i = 7$. Since $OPT[7] \neq OPT[6]$, select vault $7$.
    \item Jump $k + 1 = 3$ steps back $\Rightarrow i = 4$.
    \item $OPT[4] \neq OPT[3]$, select vault $4$.
    \item Jump back to $i = 1$.
    \item $OPT[1] \neq OPT[0]$, select vault $1$.
\end{itemize}

The selected vaults are:
\[
[1, 4, 7],
\]
with a total value of:
\[
1 + 4 + 7 = 12.
\]

\bigskip
\hrule
\bigskip

\vspace{1em}

\textbf{Algorithm 4B (Bottom-Up Iterative DP):} \\
Algorithm 4B uses an iterative version of the same recurrence.  
It builds the \texttt{OPT} table from the base case upward:
\[
OPT[0] = 0, \qquad
OPT[i] = \max_{1 \leq j \leq i} \{ OPT[i - 1],\; v_j + OPT[j - k - 1] \}.
\]
Each $OPT[i]$ represents the best possible total up to vault $i$.  
Once the table is filled, the algorithm backtracks from $i = n$ to reconstruct the chosen vaults.

\begin{algorithm}[H]
\renewcommand{\thealgorithm}{4B}
\caption{Finding Maximum Total Vault Value (Bottom-Up Iterative DP)}\label{alg:bottomup}
\begin{algorithmic}[1]
\Procedure{MaxVault-BottomUp}{$n, k, v_1, v_2, \ldots, v_n$}
    \State $OPT[0] \gets 0$
    \State $S \gets \emptyset$
    \For{$i = 1$ to $n$}
        \State $OPT[i] \gets OPT[i - 1]$
        \For{$j = 1$ to $i$}
            \State $include \gets v_j$
            \If{$j - k - 1 > 0$}
                \State $include \gets include + OPT[j - k - 1]$
            \EndIf
            \State $OPT[i] \gets \max(OPT[i], include)$
        \EndFor
    \EndFor
    \State $i \gets n$
    \While{$i > 0$}
        \If{$OPT[i] \neq OPT[i - 1]$}
            \State $S \gets S \cup \{i\}$
            \State $i \gets i - k - 1$
        \Else
            \State $i \gets i - 1$
        \EndIf
    \EndWhile
    \State \Return ($OPT[n]$, $S$)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\textbf{Step-by-step example:} \\
Suppose we have the list of vault values and $k = 2$:
\[
[1, 2, 3, 4, 5, 6, 7].
\]

We use the same recurrence relation but fill the $OPT$ table iteratively:
\[
OPT[0] = 0, \qquad
OPT[i] = \max_{1 \leq j \leq i} \{ OPT[i - 1],\; v_j + OPT[j - k - 1] \}.
\]

\textbf{Step-by-step computation:}

\begin{enumerate}
    \item $OPT[0] = 0$
    \item $OPT[1] = \max(v_1) = 1$
    \item $OPT[2] = \max(v_1, v_2) = \max(1,2) = 2$
    \item $OPT[3] = \max(v_1, v_2, v_3) = \max(1, 2, 3) = 3$
    \item $OPT[4] = \max(v_1, v_2, v_3, v_4 + OPT[1]) = \max(1, 2, 3, 4 + 1) = 5$
    \item $OPT[5] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2]) = \max(1, 2, 3, 4+1, 5 + 2) = 7$
    \item $OPT[6] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2], v_6 + OPT[3]) = \max(1, 2, 3, 4+1, 5+2, 6 + 3) = 9$
    \item $OPT[7] = \max(v_1, v_2, v_3, v_4 + OPT[1], v_5 + OPT[2], v_6 + OPT[3], v_7 + OPT[4]) = \max(1, 2, 3, 4+1, 5+2, 6+3, 7 + 5) = 12$
\end{enumerate}

At the end of the loop:
\[
OPT = [0, 1, 2, 3, 5, 7, 9, 12].
\]

\textbf{Backtracking:}
\begin{itemize}
    \item Start at $i = 7$. Since $OPT[7] \neq OPT[6]$, select vault $7$.
    \item Jump $k + 1 = 3$ steps back $\Rightarrow i = 4$.
    \item $OPT[4] \neq OPT[3]$, select vault $4$.
    \item Jump back to $i = 1$.
    \item $OPT[1] \neq OPT[0]$, select vault $1$.
\end{itemize}

The selected vaults are:
\[
[1, 4, 7],
\]
with a total value of:
\[
1 + 4 + 7 = 12.
\]

\bigskip
\hrule
\bigskip

\subsubsection{Correctness Proof}

\noindent\textbf{Theorem.}  

Let $[v_1, \ldots, v_n]$ be nonnegative vault values and $k \ge 0$ the spacing constraint.  
Define the dynamic programming recurrence:
\[
\text{OPT}[i] =
\begin{cases}
0, & \text{if } i \le 0, \\
\max_{1 \le j \le i}\{v_j + \text{OPT}[j - k - 1]\}, & \text{otherwise.}
\end{cases}
\]
Then, $\text{OPT}[n]$ equals the maximum achievable total vault value over all feasible selections.  

Using the standard backtrack rule:
\[
\text{If } \text{OPT}[i] \ne \text{OPT}[i-1] \Rightarrow \text{include } i \text{ and set } i \leftarrow i - k - 1; 
\quad \text{else set } i \leftarrow i - 1,
\]
the algorithm then returns an optimal feasible set of indices of vaults.

\vspace{1em}

\noindent\textbf{Optimality Proof.} [by induction on $i$]  

Let $\text{opt\_ind}(i)$ denote the set of feasible selections using only indices in $\{1, \ldots, i\}$,  
and let $\text{opt\_total}(i)$ be the true optimal total vault value over this prefix.  
We define $\text{opt\_total}(i) = 0$ for all $i \le 0$ so that the recurrence remains well-defined when $i - k - 1 \le 0$.

\begin{enumerate}
    \item \textbf{Base case:}  
    When there are no vaults available ($i \le 0$), the maximum total vault value is $0$,  
    meaning $\text{opt\_total}(i) = 0$.  
    The algorithm sets $\text{OPT}[i] = 0$ for all $i \le 0$, hence $\text{OPT}[i] = \text{opt\_total}(i)$ for all $i \le 0$.

    \item \textbf{Inductive step:}  
    Assume that for some $x \ge 0$, we have $\text{OPT}[j] = \text{opt\_total}(j)$ for every $j \le x$.  
    We now show that $\text{OPT}[x+1] = \text{opt\_total}(x+1)$.

    \begin{enumerate}
        \item Let $\text{opt\_ind}^*(x+1)$ denote the selection achieving $\text{opt\_total}(x+1)$.  
        To obtain the optimal value at index $x+1$, we examine all feasible choices of indices $1 \le j \le x+1$, which which is exactly what the max over $j$ in the recurrence captures, ensuring that the algorithm considers every valid way to include a vault while respecting the $k$-spacing constraint.
        
        \item There are two possible cases:
        \begin{enumerate}
            \item[(a)] \textbf{Case 1:} Vault $x+1$ is \emph{not selected}.  
            Then $\text{opt\_ind}^*(x+1)$ only includes indices up to $x$,  
            so $\text{opt\_total}(x+1) = \text{opt\_total}(x) = \text{OPT}[x]$.
            
            \item[(b)] \textbf{Case 2:} Vault $x+1$ \emph{is selected}.  
            Because of the $k$-spacing constraint, the next possible vault to include must come from  
            $\{1, \ldots, (x+1) - k - 1\}$.  
            Thus, 
            \[
            \text{opt\_total}(x+1) = v_{x+1} + \text{opt\_total}((x+1) - k - 1).
            \]
            By the induction hypothesis, $\text{opt\_total}((x+1) - k - 1) = \text{OPT}[(x+1) - k - 1]$,  
            so 
            \[
            \text{opt\_total}(x+1) = v_{x+1} + \text{OPT}[(x+1) - k - 1].
            \]
        \end{enumerate}
        
        \item Therefore, considering both cases,
        \[
        \text{opt\_total}(x+1) = \max\{\text{OPT}[x],\, v_{x+1} + \text{OPT}[(x+1) - k - 1]\} = \text{OPT}[x+1].
        \]
    \end{enumerate}
\end{enumerate}

By induction, $\text{OPT}[n] = \text{opt\_total}(n)$, so the algorithm correctly computes the maximum total value.  
Hence, the algorithm’s computed total value is \textbf{optimal}.

\vspace{1em}

\noindent\textbf{Feasibility Proof.}  

To show that the backtracking reconstruction produces a feasible set (one that satisfies the $k$-spacing constraint), we proceed as follows:

\begin{enumerate}
    \item Initialize $i = n$ and run the backtrack procedure, which ensures that the algorithm examines all selected vaults so that no feasible choice is excluded:  

    \begin{enumerate}
        \item If $\text{OPT}[i] = \text{OPT}[i-1]$, then the exclude-branch yields the optimal total at $i$, meaning that $i$ is not included in any optimal solution. We then set $i \leftarrow i - 1$.

        \item If $\text{OPT}[i] \neq \text{OPT}[i-1]$, then by the recurrence relation we have $\text{OPT}[i] = v_i + \text{OPT}[i-k-1]$.  
        Thus, some optimal solution includes vault $i$. We record index $i$ and then set $i \leftarrow i - k - 1$.


        \item Because of this jump by $k + 1$ positions, no index within the interval $[i - k, \ldots, i - 1]$ can be included in the solution.  
        Therefore, all selected indices differ by at least $k + 1$.
    \end{enumerate}
 
\end{enumerate}

Hence, the reconstructed solution satisfies the $k$-spacing constraint and is feasible.  
At each step, the inclusion or exclusion of $i$ aligns with the recurrence defining $\text{OPT}$, ensuring the recorded indices yield exactly $\text{OPT}[n]$.

\vspace{1em}

\noindent\textbf{Conclusion:}  
Both the feasibility and optimality proofs demonstrate that the dynamic programming algorithm always yields a valid (feasible) and optimal vault selection under the $k$-spacing constraint.  
Therefore, the algorithm is \textbf{correct}.




\subsubsection{Runtime Analysis}

Let $n$ be the number of vaults.

\textbf{Algorithm 4A (Top-Down Recursive DP with Memoization):} \\
Each state $OPT[i]$ is computed at most once because of memoization.  
For each $i$, the algorithm loops over all possible $j$ from $1$ to $i$ to compute
\[
OPT[i] = \max_{1 \leq j \leq i} \{ v_j + OPT[j - k - 1] \}.
\]
The inner loop therefore runs in $O(i)$ time for each $i$, and since $i$ ranges from $1$ to $n$, the total time is:

\[
T(n) = \sum_{i=1}^{n} O(i) = \sum_{i=1}^{n} O(n) = O(n^2).
\]

Memoization ensures that no subproblem is recomputed, so the total number of function calls is $O(n)$, and the dominant cost arises from the nested loop over $j$.

\vspace{1em}

\textbf{Algorithm 4B (Bottom-Up Iterative DP):} \\
The bottom-up version explicitly computes all $n$ states iteratively using the same nested structure.  
For each $i$ from $1$ to $n$, it iterates through all $j$ from $1$ to $i$, resulting in the same cost:

\[
T(n) = \sum_{i=1}^{n} O(i) = \sum_{i=1}^{n} O(n) = O(n^2).
\]

Thus, both algorithms have:

\[
T(n) = \Theta(n^2)
\]

In practice, Algorithm 4B would perform faster because of sequential memory access and the absence of a recursive overhead while Algorithm 4A provides a more intuitive top-down structure with memoization.


\subsection{Algorithm 5}
\subsubsection{Description}

We can solve the collecting treasure problem efficiently using a dynamic programming approach similar to the optimal weighted interval scheduling problem.

The recurrence relation for this algorithm is defined as:
\[
OPT[i] =
\begin{cases}
0, & \text{if } i = 0, \\[6pt]
\max \{ OPT[i - 1],\; v_i + OPT[i - k - 1] \}, & \text{otherwise.}
\end{cases}
\]

For each vault index $i$, the algorithm compares two possible choices:
\begin{enumerate}
    \item \textbf{Exclude} vault $i$: take the optimal value up to the previous vault, $OPT[i - 1]$.
    \item \textbf{Include} vault $i$: take the vault’s value $v_i$ plus the best value achievable $k$ vaults earlier, $OPT[i - k - 1]$.
\end{enumerate}

If $i - k - 1 \leq 0$, the algorithm only compares $v_i$ and $OPT[i - 1]$.  
Each $OPT[i]$ thus stores the maximum achievable total value considering vaults $1$ through $i$.  

Once the table is filled, the algorithm backtracks from the end of the array to reconstruct the indices of the chosen vaults.  
Whenever the maximum value changes between $OPT[i]$ and $OPT[i - 1]$, vault $i$ is included, and the index jumps $k + 1$ steps backward.

\begin{algorithm}[H]
\renewcommand{\thealgorithm}{5}
\caption{Finding Maximum Total Vault Value (Iterative DP)}\label{alg:5}
\begin{algorithmic}[1]
\Procedure{MaxVault}{$n, k, v_1, v_2, \ldots, v_n$}
    \State $OPT[0] \gets 0$
    \State $S \gets \emptyset$ \Comment{set of selected vault indices}
    \For{$i = 1$ to $n$}
        \State $include \gets v_i$
        \If{$i - k - 1 > 0$}
            \State $include \gets include + OPT[i - k - 1]$
        \EndIf
        \State $OPT[i] \gets \max(OPT[i - 1],\; include)$
    \EndFor
    \State $i \gets n$
    \While{$i > 0$}
        \If{$OPT[i] \neq OPT[i - 1]$}
            \State $S \gets S \cup \{i\}$
            \State $i \gets i - k - 1$
        \Else
            \State $i \gets i - 1$
        \EndIf
    \EndWhile
    \State \Return ($OPT[n]$, $S$)
\EndProcedure
\end{algorithmic}
\end{algorithm}
\textbf{Step-by-step example:} \\
Suppose we have the list of vault values and $k = 2$:
\[
[1, 2, 3, 4, 5, 6, 7].
\]

We fill the $OPT$ table iteratively using the recurrence relation.

\begin{enumerate}
    \item $OPT[0] = 0$
    \item $OPT[1] = \max(OPT[0], v_1) = \max(0,1) = 1$
    \item $OPT[2] = \max(OPT[1], v_2) = \max(1,2) = 2$
    \item $OPT[3] = \max(OPT[2], v_3) = \max(2, 3) = 3$
    \item $OPT[4] = \max(OPT[3], v_4 + OPT[1]) = \max(3, 4 + 1) = 5$
    \item $OPT[5] = \max(OPT[4], v_5 + OPT[2]) = \max(5, 5 + 2) = 7$
    \item $OPT[6] = \max(OPT[5], v_6 + OPT[3]) = \max(7, 6 + 3) = 9$    
    \item $OPT[7] = \max(OPT[6], v_7 + OPT[4]) = \max(9, 7 + 5) = 12$
\end{enumerate}

At the end of the loop:
\[
OPT = [0, 1, 2, 3, 5, 7, 9, 12].
\]

\textbf{Backtracking:}
\begin{itemize}
    \item Start at $i = 7$. Since $OPT[7] \neq OPT[6]$, select vault $7$.
    \item Jump $k + 1 = 3$ steps back to $i = 4$.
    \item $OPT[4] \neq OPT[3]$, select vault $4$.
    \item Jump back to $i = 1$.
    \item $OPT[1] \neq OPT[0]$, select vault $1$.
\end{itemize}

The selected vaults are:
\[
[1, 4, 7],
\]
with a total value of $1 + 4 + 7 = 12$.


\subsubsection{Correctness Proof}

\noindent\textbf{Theorem.}  

Let $[v_1, \ldots, v_n]$ be nonnegative vault values and $k \ge 0$ the spacing constraint.  
Define the recurrence:
\[
\text{OPT}[i] =
\begin{cases}
0, & \text{if } i \le 0, \\
\max\{\text{OPT}[i-1],\, v_i + \text{OPT}[i - k - 1]\}, & \text{otherwise.}
\end{cases}
\]
Then, $\text{OPT}[n]$ equals the maximum achievable total value over all feasible selections.  

Using the standard backtrack rule:
\[
\text{If } \text{OPT}[i] \ne \text{OPT}[i-1] \Rightarrow \text{include } i \text{ and set } i \leftarrow i - k - 1; 
\quad \text{else set } i \leftarrow i - 1,
\]
the algorithm then returns an optimal feasible set of vault indices.

\vspace{1em}

\noindent\textbf{Optimality Proof.} [by induction on $i$]  

Let $\text{opt\_ind}(i)$ denote the set of feasible selections using only indices from $\{1, \ldots, i\}$,  
and let $\text{opt\_total}(i)$ denote the true optimal total vault value for this prefix.  
We define $\text{opt\_total}(i) = 0$ for all $i \le 0$ so that the recurrence is well-defined when $i - k - 1 \le 0$.

\begin{enumerate}
    \item \textbf{Base case:}  
    When there are no vaults available ($i \le 0$), the maximum possible total vault value is $0$,  
    i.e., $\text{opt\_total}(i) = 0$.  
    The algorithm sets $\text{OPT}[i] = 0$ for $i \le 0$, hence $\text{OPT}[i] = \text{opt\_total}(i)$ for all $i \le 0$.

    \item \textbf{Inductive step:}  
    Assume for some $x \ge 0$ that $\text{OPT}[j] = \text{opt\_total}(j)$ for every $j \le x$.  
    We now show that $\text{OPT}[x+1] = \text{opt\_total}(x+1)$.

    \begin{enumerate}
        \item Let $\text{opt\_ind}^*(x+1)$ be the selection achieving $\text{opt\_total}(x+1)$.  
        \item To achieve the optimal total at index $x+1$, there are two possible cases, which are defined within the recurrence relation:

        \begin{enumerate}
            \item[(a)] \textbf{Case 1:} Vault $x+1$ is \emph{not selected}.  
            Then $\text{opt\_ind}^*(x+1)$ only includes indices up to $x$,  
            so the total value is $\text{opt\_total}(x+1) = \text{opt\_total}(x) = \text{OPT}[x]$.

            \item[(b)] \textbf{Case 2:} Vault $x+1$ \emph{is selected}.  
            Because of the $k$-spacing constraint, the next vault that can be chosen must lie within  
            $\{1, \ldots, (x+1) - k - 1\}$.  
            Thus, 
            \[
            \text{opt\_total}(x+1) = v_{x+1} + \text{opt\_total}((x+1) - k - 1).
            \]
            By the induction hypothesis,  
            $\text{opt\_total}((x+1) - k - 1) = \text{OPT}[(x+1) - k - 1]$,  
            so
            \[
            \text{opt\_total}(x+1) = v_{x+1} + \text{OPT}[(x+1) - k - 1].
            \]
        \end{enumerate}

        \item Therefore, combining both cases result in the same expression as the recurrence equation:
        \[
        \text{opt\_total}(x+1) = \max\{\text{OPT}[x],\, v_{x+1} + \text{OPT}[(x+1) - k - 1]\} = \text{OPT}[x+1].
        \]
    \end{enumerate}
\end{enumerate}

By induction, $\text{OPT}[n] = \text{opt\_total}(n)$,  
so the algorithm correctly computes the optimal total value.  
Therefore, the computed total by this algorithm is \textbf{optimal}.

\vspace{1em}

\noindent\textbf{Feasibility Proof.}  

To show that the backtracking reconstruction produces a feasible set (one that satisfies the $k$-spacing constraint), we proceed as follows:

\begin{enumerate}
    \item Initialize $i = n$ and run the backtrack procedure:  

    \begin{enumerate}
        \item If $\text{OPT}[i] = \text{OPT}[i-1]$, then the exclude-branch yields the optimal total at $i$, meaning that $i$ is not included in any optimal solution. We then set $i \leftarrow i - 1$.

        \item If $\text{OPT}[i] \neq \text{OPT}[i-1]$, then by the recurrence relation we have $\text{OPT}[i] = v_i + \text{OPT}[i-k-1]$.  
        Thus, some optimal solution includes vault $i$. We record index $i$ and then set $i \leftarrow i - k - 1$.


        \item Because of this jump by $k + 1$ positions, no index within the interval $[i - k, \ldots, i - 1]$ can be included in the solution.  
        Therefore, all selected indices differ by at least $k + 1$.
    \end{enumerate}
 
\end{enumerate}

Hence, the reconstructed solution satisfies the $k$-spacing constraint and is feasible.  
At each step, the inclusion or exclusion of $i$ aligns with the recurrence defining $\text{OPT}$, ensuring the recorded indices yield exactly $\text{OPT}[n]$. 

\vspace{1em}

\noindent\textbf{Conclusion:}  
Both the feasibility and optimality proofs show that the dynamic programming algorithm always produces a valid (feasible) and optimal vault selection under the $k$-spacing constraint.  
Therefore, the algorithm is \textbf{correct}.


\subsubsection{Runtime Analysis}

Let $n$ be the number of vaults.  
The algorithm fills the $OPT$ array using a single loop that runs from $1$ to $n$:
\[
\texttt{for i in range(1, n + 1):}
\]
Inside the loop, the algorithm performs a constant number of operations:
one addition, one comparison, and at most one conditional check.

Thus, the time complexity of filling the $OPT$ table is:
\[
T_1(n) = O(n).
\]

The backtracking loop starts from $i = n$ and decrements $i$ by at least $1$ each iteration until reaching $0$.  
Therefore, in the worst case, the backtracking requires at most $n$ steps:
\[
T_2(n) = O(n).
\]

Combining both phases:
\[
T(n) = T_1(n) + T_2(n) = O(n) + O(n) = O(2n) = O(n).
\]

Hence, the overall time complexity is:
\[
T(n) = \Theta(n).
\]

Since each iteration takes constant time and both loops run at most $n$ times, this is the most efficient possible solution for this problem.


\section{Experimental Comparative Study}

\subsection{Experimental Setup}

This experimental comparative study was conducted using the \texttt{time} module in Python to measure the runtime of each algorithm.

A \textbf{start time} and \textbf{end time} variable were recorded and subtracted to track the total elapsed running time of each program, measured in seconds.

The following Python function \texttt{test\_runtimes()} was used for tracking the runtimes:

\vspace{1em}

\begin{algorithmic}[1]
\State \textbf{Input:} spacing parameter $k$, list of input sizes $ns$
\State \textbf{Output:} runtime measurements for each $n$
\For{each $n$ in $ns$}
    \State Generate a list $values$ of length $n$ with random integers between 1 and 1000
    \State $start\_time \gets$ current time
    \State Run the algorithm \texttt{program(n, k, values)} to obtain $(\text{max\_value}, \text{chosen})$
    \State $elapsed \gets$ current time $-$ $start\_time$
    \State Print $n$ and $elapsed$
\EndFor
\end{algorithmic}

\vspace{1em}

\noindent\textbf{Choice of input sizes:} 

Due to the differences in time complexity of the algorithms, different ranges of input sizes were chosen to produce meaningful visualizations of runtime growth:

\begin{itemize}
    \item \textbf{Program3 O($2^n$):} $n = 10, 20, 30, 40, 50$. Because Program3 has exponential time complexity, larger values of $n$ would result in prohibitively long runtimes. Using small, uniformly spaced values of $n$ allows us to observe the rapid growth of the runtime without exceeding practical computational limits.
    \item \textbf{Program4A and Program4B O($n^2$):} $n = 100, 200, 300, 400, 500$. Quadratic time complexity allows for moderately larger input sizes while still keeping runtime manageable. Uniformly spaced $n$ values are chosen to accurately visualize the quadratic growth pattern.
    \item \textbf{Program5 O($n$):} $n = 10000, 20000, 30000, 40000, 50000$. Linear time complexity enables testing on very large inputs, ensuring that the growth trend is clearly observable. Uniform spacing of $n$ helps in producing a consistent visualization of runtime as $n$ increases.
\end{itemize}

These choices adhere to the goals of the experimental study:

\begin{enumerate}
    \item \textbf{High enough values of $n$:} Each algorithm is tested on input sizes large enough to capture meaningful differences in runtime growth.
    \item \textbf{Uniform distribution of $n$:} The selected input sizes are evenly spaced to prevent misleading visualizations of the runtime trend.
\end{enumerate}

This setup ensures that the growth of runtime for each algorithm can be visualized accurately and compared across different complexities.


\subsection{Plot 3}

\begin{filecontents}{p3.dat}
X   Points   Program3
1   10    0.000000
2   20    0.000920
3   30    0.020212
4   40    0.263594
5   50    6.202015
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xticklabels from table={p3.dat}{Points},
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east}
]
\addplot[blue,thick,mark=square*] table [y=Program3,x=X]{p3.dat};
% \addlegendentry{Program 3}
\end{axis}
\end{tikzpicture}
\caption{Program3 vs. Input Size}
\label{plot1}
\end{figure}

\noindent As expected from the exponential time complexity of Program3 O($2^n$), the runtime grows extremely rapidly as $n$ increases. The data illustrates the following trends:

\begin{itemize}
    \item For small $n$ (10--20), the runtime is negligible, almost zero seconds.  
    \item As $n$ increases to 30, the runtime begins to grow noticeably (0.02 seconds), reflecting the first signs of exponential growth.  
    \item By $n = 40$, the runtime increases more than tenfold compared to $n = 30$, showing the steep exponential curve characteristic of O($2^n$) algorithms.  
    \item At $n = 50$, the runtime jumps dramatically to over 6 seconds. This shows the limitations of exponential algorithms even on moderately sized inputs.  
\end{itemize}

\noindent This trend reflects the theoretical expectation: exponential algorithms quickly become infeasible as the input size grows, and small increases in $n$ lead to large increases in runtime. The results justify the choice of small input sizes for Program3 in the experimental setup to produce meaningful, measurable runtimes.

\subsection{Plot 4}

\begin{filecontents}{p4A.dat}
X   Points   Program4A
1   100    0.000000
2   200    0.018002
3   300    0.013909
4   400    0.028095
5   500    0.034267
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xticklabels from table={p4A.dat}{Points},
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east}
]
\addplot[red,thick,mark=square*] table [y=Program4A,x=X]{p4A.dat};
% \addlegendentry{Program 3}
\end{axis}
\end{tikzpicture}
\caption{Program4A vs. Input Size}
\label{plot1}
\end{figure}

\noindent The runtime trend for Program4A reflects its quadratic time complexity, O($n^2$):

\begin{itemize}
    \item For smaller input sizes ($n = 100, 200$), runtimes are very low, in the milliseconds.  
    \item As $n$ increases, the runtime gradually grows, consistent with O($n^2$) growth, though the increase is moderate compared to exponential growth.  
    \item The trend remains smooth, with runtimes increasing roughly proportionally to the square of $n$. Minor fluctuations are likely due to system timing variations or random input values.  
\end{itemize}

\noindent Overall, the results demonstrate that Program4A scales efficiently for moderate input sizes, allowing much larger $n$ than Program3.

\subsection{Plot 5}

\begin{filecontents}{p4B.dat}
X   Points   Program4B
1   100    0.000926
2   200    0.004995
3   300    0.007158
4   400    0.013426
5   500    0.035091
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xticklabels from table={p4B.dat}{Points},
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east}
]
\addplot[orange,thick,mark=square*] table [y=Program4B,x=X]{p4B.dat};
% \addlegendentry{Program 3}
\end{axis}
\end{tikzpicture}
\caption{Program4B vs. Input Size}
\label{plot1}
\end{figure}

\noindent Similar to Program4A, Program4B exhibits quadratic time complexity:

\begin{itemize}
    \item Runtimes increase as input size grows from 100 to 500, reflecting O($n^2$) scaling.  
    \item Growth is more noticeable at larger input sizes, particularly from $n = 400$ to $n = 500$.  
    \item Small fluctuations in runtime are present, likely caused by system load or the random generation of input values, but the overall trend follows the expected quadratic pattern.  
\end{itemize}

\noindent These results confirm that Program4B can handle moderately large inputs efficiently, much larger than what would be feasible for Program3.

\subsection{Plot 6}

\begin{filecontents}{p5.dat}
X   Points   Program5
1   10000    0.004000
2   20000    0.001142
3   30000    0.021757
4   40000    0.022194
5   50000    0.034442
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xticklabels from table={p5.dat}{Points},
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east}
]
\addplot[green,thick,mark=square*] table [y=Program5,x=X]{p5.dat};
% \addlegendentry{Program 3}
\end{axis}
\end{tikzpicture}
\caption{Program5 vs. Input Size}
\label{plot1}
\end{figure}

\noindent Program5 exhibits linear time complexity, O($n$), which is reflected in the runtimes:

\begin{itemize}
    \item Runtimes remain very low even for large input sizes from $n = 10{,}000$ to $n = 50{,}000$, consistent with linear scaling.  
    \item The runtime for $n=20{,}000$ appears lower than $n=10{,}000$, likely due to small system-level timing variations or background processes; however, the overall trend increases roughly linearly with $n$.  
    \item As $n$ increases, the runtime gradually rises, reaching 0.034 seconds at $n=50{,}000$, which is still very fast and confirms the efficiency of Program5 for large datasets.  
\end{itemize}

\noindent These results validate that Program5 scales efficiently with input size and justify testing it on significantly larger datasets compared to Program3 and Program4A/4B.

\subsection{Plot 7}

\begin{filecontents}{p3.dat}
X   Points   Program3
1   10      0.000000
2   20      0.000920
3   30      0.020212
4   40      0.263594
5   50      6.202015
\end{filecontents}

\begin{filecontents}{p4A.dat}
X   Points   Program4A
1   100     0.000000
2   200     0.018002
3   300     0.013909
4   400     0.028095
5   500     0.034267
\end{filecontents}

\begin{filecontents}{p4B.dat}
X   Points   Program4B
1   100     0.000926
2   200     0.004995
3   300     0.007158
4   400     0.013426
5   500     0.035091
\end{filecontents}

\begin{filecontents}{p5.dat}
X   Points   Program5
1   10000    0.004000
2   20000    0.001142
3   30000    0.021757
4   40000    0.022194
5   50000    0.034442
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east},
    xmode=log,
    ymode=log,
    legend style={
        at={(1.05,0.5)},    % position outside to the right
        anchor=west,        % align legend to the west (left) of this position
        draw=black,         % add a border around the legend
        fill=white           % optional: background color
    },
]

% Program 3
\addplot[blue,thick,mark=*] table [y=Program3,x=X]{p3.dat};
\addlegendentry{Program3}

% Program 4A
\addplot[red,thick,mark=*] table [y=Program4A,x=X]{p4A.dat};
\addlegendentry{Program4A}

% Program 4B
\addplot[orange,thick,mark=*] table [y=Program4B,x=X]{p4B.dat};
\addlegendentry{Program4B}

% Program 5
\addplot[green,thick,mark=*] table [y=Program5,x=X]{p5.dat};
\addlegendentry{Program5}

\end{axis}
\end{tikzpicture}
\caption{Overlay of Programs 3, 4A, 4B, 5 vs. Input Size}
\label{fig:overlay}
\end{figure}

\noindent The results of the overlay are as follows:
\begin{itemize}
    \item Program3 exhibits exponential growth (O($2^n$)), with runtimes increasing rapidly from near-zero at $n=10$ to over 6 seconds at $n=50$, making it practical only for small inputs. 
    \item Programs 4A and 4B display quadratic growth (O($n^2$)), with moderate runtime increases as $n$ grows from 100 to 500. 
    \item Program5 scales linearly (O($n$)), handling very large input sizes efficiently from $n=10{,}000$ to $n=50{,}000$, with runtimes remaining low. 
\end{itemize}


\noindent The overlay plot clearly illustrates the differing scaling behaviors and justifies the choice of input sizes for each algorithm.


\subsection{Plot 8}

\begin{filecontents}{p4A.dat}
X   Points   Program4A
1   100     0.000000
2   200     0.018002
3   300     0.013909
4   400     0.028095
5   500     0.034267
\end{filecontents}

\begin{filecontents}{p4B.dat}
X   Points   Program4B
1   100     0.000926
2   200     0.004995
3   300     0.007158
4   400     0.013426
5   500     0.035091
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel=Running Time (seconds),
    xtick=data,
    legend style={at={(0.97,0.03)},anchor=south east},
    legend style={
        at={(1.05,0.5)},    % position outside to the right
        anchor=west,        % align legend to the west (left) of this position
        draw=black,         % add a border around the legend
        fill=white           % optional: background color
    },
]

% Program 4A
\addplot[red,thick,mark=*] table [y=Program4A,x=X]{p4A.dat};
\addlegendentry{Program4A}

% Program 4B
\addplot[orange,thick,mark=*] table [y=Program4B,x=X]{p4B.dat};
\addlegendentry{Program4B}

\end{axis}
\end{tikzpicture}
\caption{Overlay of Programs 4A and 4B vs. Input Size}
\label{fig:overlay}
\end{figure}

\noindent Observations from the overlay:
\begin{itemize}
    \item Both Programs 4A and 4B show quadratic scaling (O($n^2$)), as expected from their algorithmic complexity.
    \item Runtime increases moderately as $n$ grows from 100 to 500, with Program4B generally slightly faster than Program4A for most input sizes.
    \item The trend confirms that quadratic algorithms remain feasible for small-to-moderate inputs, but would scale poorly for much larger $n$.
\end{itemize}

\subsection{Plot 9}

\begin{filecontents}{p1.dat}
X   Points   Algorithm1
1   10000    0.00038654
2   20000    0.00064708
3   30000    0.00095906
4   40000    0.00137282
5   50000    0.00188342
\end{filecontents}

\begin{filecontents}{p5_output.dat}
X   Points   Program5
1   10000    0.004000
2   20000    0.001142
3   30000    0.021757
4   40000    0.022194
5   50000    0.034442
\end{filecontents}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    xlabel=Number of Elements,
    ylabel={Relative Output Difference $(h_g - h_o)/h_o$},
    xticklabels from table={p1.dat}{Points},
    xtick=data,
    legend style={
        at={(1.05,0.5)},
        anchor=west,
        draw=black,
        fill=white
    },
]

% Compute relative difference for Program5
\addplot[red,thick,mark=*] table[x=X, y expr={(\thisrow{Algorithm1}-\thisrow{Program5})/\thisrow{Program5}}]{%
X Algorithm1 Program5
1 0.00038654 0.004000
2 0.00064708 0.001142
3 0.00095906  0.021757
4 0.00137282 0.022194
5 0.00188342 0.034442
};
\addlegendentry{Algorithm1 vs Program5}

\end{axis}
\end{tikzpicture}
\caption{Relative output quality comparison: Algorithm1 vs Program5}
\label{fig:quality_compare}
\end{figure}

\noindent The relative output difference between Algorithm1 and Program5, computed as $(h_g - h_o)/h_o$, exhibits the following trends:

\begin{itemize}
    \item For smaller input sizes ($n=10{,}000$ and $n=20{,}000$), the relative difference is quite low, around $-0.903$ and $-0.433$, indicating that Algorithm1 produces slightly lower outputs than Program5 in relative terms.  
    \item As the input size increases, the relative differences fluctuate, with negative values at $n=10{,}000$ and $n=20{,}000$ and smaller negative differences at $n=30{,}000$ to $n=50{,}000$. This suggests that Algorithm1 consistently produces outputs of comparable magnitude to Program5, though slightly smaller.  
    \item The magnitude of the relative difference generally remains under 1 for all tested input sizes, indicating that Algorithm1 maintains reasonable accuracy compared to Program5 across the tested range.  
    \item These results highlight that while Program5 may achieve slightly higher absolute values in this metric, Algorithm1 still produces outputs of similar scale, validating its effectiveness for large datasets.  
\end{itemize}

\noindent Overall, Algorithm1 provides competitive output quality relative to Program5, with relative differences small enough to justify using Algorithm1 when its computational advantages are considered.


\subsection{Observations/Comments}

\noindent From the experimental results, several trends are revealed:

\begin{itemize}
    \item Program3 exhibits exponential growth, making it impractical for inputs larger than $n=50$.
    \item Programs 4A and 4B scale quadratically and remain feasible for moderate input sizes, with 4B slightly faster on average than 4A.
    \item Program5 produces linear scaling, efficiently handling very large inputs up to $n=50{,}000$ with low runtimes.
    \item Algorithm1 produces outputs of comparable quality to Program5, with small relative differences across all tested input sizes, confirming its effectiveness.
    \item The overlay plots clearly illustrate the different scaling behaviors, justifying the choice of different input ranges for each algorithm.
\end{itemize}

\noindent Overall, the experiments highlight the trade-offs between runtime efficiency and output quality, and show that linear and quadratic algorithms are suitable for large-scale data, while exponential algorithms are limited to small inputs.


\section{Conclusion}

This component of the project allowed us to collaborate in developing and analyzing dynamic programming and efficient algorithms for selecting vaults under spacing constraints. 

\vspace{1em}

Program 3 was the most challenging to implement due to its exponential time complexity, O($2^n$). While the algorithm itself was straightforward, testing it for larger inputs quickly became impractical. This exercise helped us understand the limitations of naive recursive solutions and reinforced the importance of considering time complexity when designing algorithms. Through careful testing and small input sizes, we were able to verify correctness and gain insight into the exponential growth of the runtime.

\vspace{1em}

Programs 4A and 4B were implemented using dynamic programming with quadratic time complexity, O($n^2$). Implementing these algorithms required careful attention to the recurrence relations and the $k$-spacing constraints. Comparing the two variants, we observed slight differences in runtime efficiency. Proving correctness and feasibility involved structured induction and backtracking proofs, reinforcing our understanding of formal algorithmic reasoning.

\vspace{1em}

Program 5 was the most efficient, with linear time complexity, O($n$). Developing this algorithm demonstrated how dynamic programming can be optimized to achieve scalable performance on very large datasets. Testing on inputs up to $n = 50{,}000$ confirmed the algorithm’s efficiency and correctness, and the overlay plots illustrated the dramatic improvement over less efficient algorithms.

\vspace{1em}

Overall, this portion of the project helped us practice designing, implementing, and proving dynamic programming algorithms with varying time complexities. It strengthened our ability to analyze runtime and output quality, and improved our collaborative skills in developing efficient, correct, and feasible algorithms for a range of input sizes.

\end{document}